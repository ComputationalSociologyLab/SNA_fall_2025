
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width=750)
```

```{r message=F, warning=F}
#manual installation for package "ldatuning"
#install.packages("devtools")
#library(devtools)
#install_github("nikita-moor/ldatuning")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width=750)
```

# Two-mode Networks: Affilitations and Dualities {#ch11-Two-mode-Networks}

This tutorial walks through the analysis of two-mode (affiliation) data in R. We will cover how to plot two-mode network data using the **igraph** package, how to transform it into one mode data and how to calculate traditional measures, like centrality. The tutorial will draw directly on many of the previous tutorials, including [Chapter 5](#ch5-Network-Visualization-R) (visualization), [Chapter 8](#ch8-Network-Cohesion-Communities-R) (cohesion and groups), and [Chapter 9](#ch9-Network-Centrality-Hierarchy-R) (centrality). We will work with affiliation data collected by Daniel McFarland on student extracurricular affiliations. It is a longitudinal data set, with 3 waves - 1996, 1997, 1998, although we will only use the first wave, 1996. We consider dynamics on two-mode networks in [Chapter 13, Part 3](#ch13-Two-mode-Network-Models-ERGM-STERGM-R) (statistical models for two-mode networks). The data consist of students (anonymized) and the student clubs in which they are members (e.g., National Honor Society, wrestling team, cheerleading squad, etc.). The data thus allow us to capture the duality of social life in the school, as students are linked by being in the same clubs, and clubs are linked by sharing the same students.

Substantively, we are motivated by the following questions: a) Which student clubs serve to integrate the school and which are more peripheral?  b) Which student clubs tend to share members at high rates? c) What is the shared feature, or theme, that brings these clubs together in a cluster?  Overall, we are interested in the manner in which membership in clubs serves to integrate (or divide) the school. 

## Example Affiliation Data

We will use the **igraph** package for this tutorial.

```{r message=F, warning=F}
library(igraph) 
```

Our data are stored in three files: an affiliation matrix (showing which students are part of which clubs), a student attribute file (showing the attributes of the students), and a club attribute file (showing the attributes of the clubs). Let's read in the affiliation file from a URL:

```{r}
url1 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/affiliations_1996.txt"

affiliations96 <- read.delim(file = url1, check.names = FALSE)
```

We need to preserve the original column names for labeling our visualizations so we use the "check.names = FALSE" argument. The affiliation matrix is organized with students on the rows and clubs on the columns. 

```{r}
dim(affiliations96)
```

We can see that there are 1295 students and 91 clubs. The affiliation matrix is based on a series of club membership dummy variables, coded "1" for membership, "0" for no membership. Let's take a look at the data (just the first 6 rows and 6 columns):

```{r}
affiliations96[1:6, 1:6] 
```

We can see, for example, that the second student in the data is part of the marching band while the first student is not. Note that the rownames of the matrix capture the ids of the students. To get a sense of the data, let’s look at the affiliations for the first student in 1996. Here we grab the first row in the affiliation data and locate the 1s in that row:

```{r}
affils_student1 <- affiliations96[1, ]
affils_student1[which(affils_student1 == 1)]
```

We can see that student 1 was a member of the NHS, Spanish Club, Spanish NHS, Theatre Productions and Thespian Society. And now we will read in the attribute data. We have two files, one for the students and one for the clubs.

```{r}
url2 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/attributes_students.txt"

attributes_students <- read.delim(file = url2, stringsAsFactors = FALSE)

url3 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/attributes_clubs.txt"

attributes_clubs <- read.delim(file = url3, stringsAsFactors = FALSE)
```

The student data frame only includes students (1295 rows) and includes student specific attributes. The student specific attributes are: 

-  gender (male, female) 
-  grade96 (grade in 1996) 
-  grade97 (grade in 1997) 
-  race (Asian, black, Hispanic, Native American, white). 

Let's take a look at the first five rows of our student data frame.

```{r}
attributes_students[1:5, ]
```

The club data frame only includes clubs (91 rows) and includes club specific attributes.The main club attributes of interest are:  

-  club_type_detailed (Academic Interest, Academic Competition, Ethnic Interest, Individual Sports, Leadership, Media, Performance Art, Service, Team Sports) 
-  club_profile (how much attention does the club get? low, moderate, high, very high) 
-  club_feeder (does club feed another club, like 8th grade football to 9th grade football?)
-  club_type_gender (is club mixed gender, just boys or just girls?)
-  club_type_grade (which grades, if any, is club restricted to?). 

Now, let's take a look at the first five rows of our club data frame. 

```{r}
attributes_clubs[1:5, ]
```

Note that the student attribute data frame must be sorted in the same order as the rows of the affiliation matrix, while the club attribute data frame must be sorted in the same order as the columns of the affiliation matrix. This is already done for our data.

We will now make a combined data frame, that puts together the student attributes with the club attributes. This will be useful when constructing the igraph object below. We will proceed by stacking the student data frame on top of the club data frame (as the igraph object will order the nodes by rows and then columns). For student specific attributes, we will put in NAs for the clubs; for club specific attributes, we will put in NAs for the students. For example, it does not make sense to think of a club having a 'race', so we put in an NA for race. There are also a few attributes that pertain to both students and clubs and here there are no structural NAs, as both node types have meaningful values. These include `ids` (a unique identifier), `type` (student or club), `missing96` (is node missing in 1996, 0 = no; 1 = yes), `missing97` (is node missing in 1997, 0 = no; 1 = yes).

Let's start with those variables that have meaningful values for both students and clubs. In this case, we simply need to stack the values for students on top of the values for clubs.

```{r}
shared_var_names <- c("ids", "type", "missing96", "missing97")

shared <- rbind(attributes_students[, shared_var_names], 
                attributes_clubs[, shared_var_names])
```

And now let's move to the student specific attributes, putting in NAs for the clubs. We will keep this simple and first create vectors of NAs of the right length (91), representing the values for the clubs. We will then put that together with the student values for race, gender and grade. 

```{r}
num_clubs <- nrow(attributes_clubs)
NA_dat_club <- rep(NA, num_clubs)

student_var_names <- c("race", "gender", "grade96", "grade97")

student_specific <- rbind(attributes_students[, student_var_names], 
                          data.frame(race = NA_dat_club, 
                                     gender = NA_dat_club,
                                     grade96 = NA_dat_club,
                                     grade97 = NA_dat_club))
```

And now we do the same thing for club specific attributes, putting in NAs for the students and then stacking the data together; again, with the students stacked on top of the clubs. 

```{r}
num_students <- nrow(attributes_students)
NA_dat_student <- rep(NA, num_students)

club_var_names<- c("club_type_detailed", "club_profile", 
                   "club_feeder", "club_type_gender", 
                   "club_type_grade")

club_specific <- rbind(data.frame(club_type_detailed = NA_dat_student, 
                                  club_profile = NA_dat_student,
                                  club_feeder = NA_dat_student,
                                  club_type_gender = NA_dat_student,
                                  club_type_grade = NA_dat_student), 
                       attributes_clubs[, club_var_names])
```

And now let's combine the three data frames together, column-wise. 

```{r}
attributes_students_clubs <- cbind(shared, student_specific, club_specific)
```

```{r}
head(attributes_students_clubs)
```

For the students, we correctly see NA values for the club specific columns (like `club_type_detailed`). Before we construct our igraph object, we first need to remove any students or clubs who were not in the school in the 1996 year (the focus of our analysis here). We can determine this by looking at the `missing96` column in the `attributes_students_clubs` data frame. Let's only keep those cases where `missing96` is equal to 0 (i.e., not missing). We need to do this for both the attribute data frame and the affiliation matrix. Note that we must identify the missing students and clubs separately and use that to reduce the rows and columns in the affiliation matrix. 

```{r}
not_missing <- attributes_students_clubs$missing96 == 0

is_student <- attributes_students_clubs$type == "student"
not_missing_student <- not_missing[is_student]

is_club <- attributes_students_clubs$type == "club"
not_missing_club <- not_missing[is_club]

affiliations96_nomiss <- affiliations96[not_missing_student, not_missing_club]

attributes_nomiss <- attributes_students_clubs[not_missing, ]
```

We are now in position to create our two-mode graph in igraph. The function is `graph_from_incidence_matrix()`. The main arguments are incidence (the two-mode affiliation matrix) and mode. Here we set mode to "all", telling igraph to create mutual connections between student and club.

```{r}
affil_net96 <- graph_from_incidence_matrix(incidence = affiliations96_nomiss, 
                                           mode = "all")
```

```{r}
affil_net96 
```

We can see that there are 1029 nodes and 2641 edges. Note that the two-mode, or bipartite, network has a vertex attribute called `type` that indicates the kind of node. `type` is automatically created by igraph and attached as a vertex attribute. A False corresponds to the students (rows) and True corresponds to the clubs (columns). We thus have a network of students and clubs where all ties link students to clubs (based on membership). Let’s grab the vertex attribute type from the two-mode igraph object.

```{r}
type96 <- vertex_attr(affil_net96, "type") 
```

```{r}
table(type96) 
```

We see that there are 938 students and 91 clubs in the school. And now we can go ahead and put the attributes of the nodes onto the igraph object. We will add attributes for `race`, `gender`, `grade96`, `club_type_detailed` and `club_profile`. Remember that `race`, `grade96` and `gender` are only relevant for students (clubs have an NA), while `club_type_detailed` and `club_profile` are only relevant for clubs (students have an NA). 

```{r}
affil_net96 <- set_vertex_attr(graph = affil_net96, name = "race", 
                               value = attributes_nomiss$race)

affil_net96 <- set_vertex_attr(graph = affil_net96, name = "gender", 
                               value = attributes_nomiss$gender)

affil_net96 <- set_vertex_attr(graph = affil_net96, name = "grade96", 
                               value = attributes_nomiss$grade96)

affil_net96 <- set_vertex_attr(graph = affil_net96, name = "club_type_detailed", 
                               value = attributes_nomiss$club_type_detailed)

affil_net96 <- set_vertex_attr(graph = affil_net96, name = "club_profile", 
                               value = attributes_nomiss$club_profile)
```

## Plotting the Network
Now that we have our two-mode networks constructed, let's first figure out how to create a nice plot. The **igraph** package has excellent plotting functionality that allows you to assign visual attributes to igraph objects before you plot. The alternative is to pass 20 or so arguments to the `plot.igraph()` function, which can get messy.

Each node (or "vertex") object is accessible by calling `V(g)`, and you can call (or create) a node attribute by using the `$` operator (i.e., `V(g)$name_of_attribute`).  Let's now use this notation to set the vertex color, with special attention to making graph objects slightly transparent. We'll use the `rgb()` function in R to do this. We specify the levels of red, green, and blue and "alpha-channel" (a.k.a. opacity) using this syntax: `rgb(red, green, blue, alpha)`. To return solid red, one would use this call: `rgb(red = 1, green = 0, blue = 0, alpha = 1)`. We will make nodes, edges, and labels slightly transparent so that when things overlap it is still possible to read them. You can read up on the RGB color model at http://en.wikipedia.org/wiki/RGB_color_model. In this case let’s color the students red and the affiliations green. Here we set the students red and set the opacity to .5:

```{r}
V(affil_net96)$color[type96 == FALSE] <- rgb(red = 1, green = 0, 
                                             blue = 0, alpha = .5)
```

And now we set the clubs green: 

```{r}
V(affil_net96)$color[type96 == TRUE] <- rgb(red = 0, green = 1, 
                                            blue = 0, alpha = .5) 
```

Notice that we index the `V(g)$color` object by using the `type96` variable used above. Students correspond to False in the type variable and clubs correspond to True. From here on out, we do not specify "red = ", "green = ", "blue = ", and "alpha = ". These are the default arguments (R knows the first number corresponds to red, the second to green, and so on). Now we'll set some other graph attributes:

```{r}
V(affil_net96)$label <- V(affil_net96)$name # setting label of nodes
V(affil_net96)$label.color <- rgb(0, 0, .2, .5) # set the color of the labels
V(affil_net96)$label.cex <- .5 # make the labels of the nodes smaller
V(affil_net96)$size <- 6 # set size of nodes to 6
V(affil_net96)$frame.color <- V(affil_net96)$color # set color around nodes 
```

We can also set edge attributes. We set the edge attributes using the `E()` function. Here we'll make the edges nearly transparent and slightly yellow because there will be so many edges in this graph:

```{r}
E(affil_net96)$color <- rgb(.5, .5, 0, .2)
```

We can also specify which layout we will use. In this case, we will use the Fruchterman-Reingold force-directed layout algorithm. Generally speaking, when you have a ton of edges, the Kamada-Kawai layout algorithm works well. It can, however, be slow for networks with many nodes. As a default, igraph will use the grid based implementation of the algorithm in the case of more than a thousand vertices. This implementation is faster, but can fail to produce a plot with any meaningful pattern if you have too many isolates. Also note that for more recent versions of igraph, the F-R layout has been rewritten and will produce bad looking plots if a network has more than 999 vertices. You need to fix the grid parameter to "nogrid" to fix the problem. Our network has more than 999 vertices and has many isolates. Therefore, we want to offset the default and specify that we do not want to use the grid implementation.

```{r}
layout <- layout_with_fr(affil_net96, grid = "nogrid")
```

Now, we'll open a pdf "device" on which to plot. This is just a connection to a pdf file. We will go ahead and save it to our working directory, which can be set using `setwd()` and checked using `getwd()`.

```{r results='hide'}
pdf("magact_stdnt_actvts_1996.pdf")
plot(affil_net96, layout = layout)
dev.off() 
```

```{r echo=F, fig.height=7.5, fig.width=7.5}
#This code is the same as above, and is just used to generate the plot within the generated html file. 
plot(affil_net96, layout = layout)
```

Once we break off the connection, the created plot will be saved out in the working directory, saved as "magact_stdnt_actvts_1996.pdf" (for convenience, we have also included a version of the plot in this html output).  Now, if you open the pdf output, you'll notice that you can zoom in on any part of the graph ad infinitum without losing any resolution. How is that possible in such a small file? It's possible because the pdf device output consists of data based on vectors: lines, polygons, circles, ellipses, etc., each specified by a mathematical formula that your pdf program renders when you view it. Regular bitmap or jpeg picture output, on the other hand, consists of a pixel-coordinate mapping of the image in question, which is why you lose resolution when you zoom in on a digital photograph or a plot produced as a picture.

Looking at the output, this plot is oddly reminiscent of a crescent and star, but impossible to read. Part of the problem is the way in which the layout algorithms deal with the isolates. For example, layout_with_fr will squish all of the connected nodes into the center creating a useless "hairball"-like visualization.

Let's remove all of the isolates (the crescent), change a few aesthetic features, and replot. First, we'll remove isolates, by deleting all nodes with a degree of 0, meaning that they have zero edges. In this case, this means students who are part of no clubs or clubs that have no members. Then, we'll suppress labels for students and make their nodes smaller and more transparent. Then we'll make the edges narrower and more transparent. Then, we'll replot using various layout algorithms. 

Here we identify the isolates and then remove them from the network:

```{r}
degree0 <- which(degree(affil_net96) == 0)
affil_net96_noisolates <- delete_vertices(affil_net96, degree0)
```

Here we extract the type of node for this subsetted network:

```{r}
type96_noisolates <- vertex_attr(affil_net96_noisolates, "type")
```

```{r}
table(type96_noisolates) 
```

Let's create a vector that identifies if the node is a student or not.

```{r}
is_student_type <- type96_noisolates == FALSE
```

We see that we still have 91 clubs but many students have been removed. Now, let’s tweak some of the plotting parameters. We will take off the labels for the students, change the color of the students and make the student nodes smaller.

```{r}
V(affil_net96_noisolates)$label[is_student_type] <- NA
V(affil_net96_noisolates)$color[is_student_type] <- rgb(1, 0, 0, .1)
V(affil_net96_noisolates)$size[is_student_type] <- 2
```

Here we change some of the edge attributes for the plot.

```{r}
E(affil_net96_noisolates)$color <- rgb(.5, .5, 0, .05)
```

Now let's create the plot, saving out it out as a pdf. First with Kamada-Kawai layout:

```{r results='hide'}
pdf("magact_stdnt_actvts_1996_layout_with_kk.pdf")
plot(affil_net96_noisolates, layout = layout_with_kk)
dev.off()
```

```{r echo=F, fig.height=7.5, fig.width=7.5}
#This code is the same as above, and is just used to generate the plot within the generated html file. 
plot(affil_net96_noisolates, layout = layout_with_kk)
```

We can see that the clubs are colored green and have labels, while the students are colored a transparent red and have no labels. Now with the FR layout.

```{r results='hide'}
pdf("magact_stdnt_actvts_1996_layout_with_fr.pdf")
plot(affil_net96_noisolates, layout = layout_with_fr)
dev.off()
```

```{r echo=F, fig.height=7.5, fig.width=7.5}
#This code is the same as above, and is just used to generate the plot within the generated html file. 
plot(affil_net96_noisolates, layout = layout_with_fr)
```

Note that if we ran this again we would get a slightly different figure. The nice thing about the Fruchterman-Reingold layout in this case is that it really emphasizes centrality -- the nodes that are most central are nearly always placed in the middle of the plot. More generally, these plots are nice in that they emphasize the dualistic nature of student and club (or person and affiliation). We see that certain clubs and students are placed in close proximity to each other. In this way, we get a space, where students and clubs tend to cluster together, with sets of students tending to join the same kinds of clubs. For example, we can see that eighth grade boy’s track and eighth grade boy’s cross-country tend to cluster together. They are linked in the sense that students in one club tend to be in another. From the other side, students on these teams are placed close together in the space, as they are linked through common patterns of club membership (here eighth grade boy sports). 

We can also incorporate the node attributes into the plot. Here, we will do a quick plot where the nodes are colored based on racial identity. Student will be colored by racial identity while clubs will all be green, as before. We will use the `recode()` function in the car package to set the colors of the nodes. We only want to change the color for the students, however, and so we restrict the recoding to just those cases where type is a student. 

```{r message=F, warning=F}
library(car)

race <- V(affil_net96_noisolates)$race[is_student_type]

student_node_color <- recode(race,
                             "'white' = 'red'; 
                             'Hispanic' = 'grey';
                             'Asian' = 'blue';
                             'black' = 'yellow';
                             'Native American' = 'black'")
```

And now we set the color of the nodes (both fill and frame of the circle) based on the vector of colors defined above, just changing the values for the students. 

```{r}
V(affil_net96_noisolates)$color[is_student_type] <- student_node_color 
V(affil_net96_noisolates)$frame.color[is_student_type] <- student_node_color
```

For this plot we will also take off the labels for the clubs. 

```{r}
V(affil_net96_noisolates)$label[!is_student_type] <- NA
```

And now we can go ahead and produce the plot (this time we won't save it out as a pdf). 

```{r fig.height=7.5, fig.width=7.5}
plot(affil_net96_noisolates)
```

We can see that our students are colored yellow, red, blue, etc., while the clubs are all green. At first glance, it would appear that students tend to be closest to other students of the same race (i.e., they join the same clubs), but that pockets of racial groups are often intermixed in the same region of the plot. We will return to the question of racial segregation in affiliations below. 

While the two-mode network representation is useful and intuitive, it is often easier to actually analyze the one-mode projections of the network, and that is what we turn to next. 

## Single Mode Networks
Having plotted our two-mode network for the 1996 year, we now move to an analysis that focuses on the one-mode projections of the network. We will produce two networks, one for student-to-student ties and one for club-to-club ties based on the student-to-club network analyzed above. For the student-to-student network, all of the nodes are students and we define a tie between two students based on the number of clubs they have in common. For the club-to-club network, all the nodes are clubs, and we define a tie between clubs based on the number of members they have in common. 

### Constructing One-Mode Projections
We will begin by producing the one-mode projection using matrix multiplication. This is done using R's matrix algebra commands. We first need to get the affiliation data into the matrix format. To get the one-mode representation of ties between rows (students in our example), multiply the matrix by its transpose. To get the one-mode representation of ties between columns (clubs in our example), multiply the transpose of the matrix by the matrix.  Note that you must use the matrix-multiplication operator `%*%`  rather than a simple asterisk. For the club to club network: 

```{r}
affiliations96_nomiss <- as.matrix(affiliations96_nomiss)
club_club96 <- t(affiliations96_nomiss) %*% affiliations96_nomiss 
```

```{r}
club_club96[1:5, 1:5]
```

The values in the matrix capture the number of people who are members of the row club and column club. For example, there are three students who are part of the Academic decathlon club and the Art club. Note that the diagonal tells us the number of students in that club. So, there are 33 students in the Art club. 

```{r}
dim(club_club96) 
```

The matrix is square with the number of rows and columns equal to the number of clubs, 91 in this case. If we wanted to get the student to student network, we would take the affiliation matrix and multiply it by the transpose of the affiliation matrix:

```{r}
student_student96 <- affiliations96_nomiss %*% t(affiliations96_nomiss) 
```

```{r}
student_student96[1:5, 1:5] 
```

The values capture the number of clubs that student i (on the rows) and j (on the columns) have in common. 

```{r}
dim(student_student96) 
```

Again, we have a square matrix, but this time the rows and columns are equal to the number of students in the school. We can now take those matrices and create two igraph objects. This will make it possible to plot and calculate statistics on the one-mode networks.

It is also possible to use the functions within igraph to construct the one-mode networks directly from the two-mode network, without having to do the matrix multiplication. Here we will rely on the `bipartite_projection()` function, which converts a two-mode (bipartite) network to two one-mode (unipartite) networks. 

```{r}
onemode96 <- bipartite_projection(affil_net96)
```

This is a list with two objects (`proj1` and `proj2`). The first is the student-student projection and the second is the club-club projection. Let's take a look at the club-to-club network: 

```{r}
club_net96 <- onemode96$proj2
```

Note that an edge weight has been automatically passed to the constructed graph. The weight for the club-to-club network is based on the number of shared members between i and j. This is stored as an edge attribute called `weight`. We can extract useful information from this object, including the names of the nodes, here the clubs.

```{r}
club_names <- V(club_net96)$name 
```

```{r}
length(club_names) 
```

We see there are 91 names. We can also grab the matrix from this igraph object. We include an attr argument to get the values in the matrix.

```{r}
mat <- as_adjacency_matrix(graph = club_net96, attr = "weight", sparse = F)
```

```{r}
mat[1:5, 1:5]
```

This matrix is the same as `club_club96` except for the diagonal. The diagonal here is set to 0, as igraph takes out the self-weight (number of members) when constructing the network. 

### Plotting One-Mode Projections
We now turn to plotting the single mode, club-to-club network (we could do an analogous plot for students). One of the main challenges of plotting these networks is that the one-mode projections of two-mode networks tend to be quite dense. We will thus need to pay special attention to the coloring of the edges, noting that the edges themselves are weighted. We will begin by setting vertex attributes, making sure to make them slightly transparent by altering the alpha input to the `rgb()` function.

```{r}
V(club_net96)$label.color <- rgb(0, 0, .2, .8)
V(club_net96)$label.cex <- .60 
V(club_net96)$size <- 6
V(club_net96)$color <- rgb(0, 0, 1, .3)
V(club_net96)$frame.color <- V(club_net96)$color 
```

We will also set the edge opacity/transparency as a function of how many students each club has in common (the weight of the edge that connects the two clubs). We use the `log()` function to make sure all transparencies are on relatively the same scale, then divide by twice the maximum edge weight to get them on a scale from about .1 and .5.

```{r}
egalpha <- log1p(E(club_net96)$weight) / max(log1p(E(club_net96)$weight) * 2)
E(club_net96)$color <- rgb(.25, .75, 0, egalpha)
```

For illustrative purposes, let's compare how the LGL (layout generator for larger graphs) and Fruchterman-Reingold algorithms render this graph:

```{r results='hide'}
pdf("magact_stdnt_actvts_1996_clubs.pdf")
plot(club_net96, main = "layout_with_lgl", layout = layout_with_lgl)
plot(club_net96, main = "layout_with_fr", layout = layout_with_fr) 
dev.off()

```


```{r echo=F, fig.height=7.5, fig.width=7.5}
#This code is the same as above, and is just used to generate the plot within the generated html file. 
plot(club_net96, main = "layout_with_lgl", layout = layout_with_lgl)
plot(club_net96, main = "layout_with_fr", layout = layout_with_fr) 
```

Be sure to go to the second page in the pdf to see the FR layout. You might like the LGL layout for this graph, because the center of the graph is very busy if you use the Fruchterman-Reingold layout. 

The plot looks okay but is pretty dense, making it harder to interpret. In this next plot, we will pare things down a bit, weighting the edges to produce a simpler plot. We will make the width of the edges proportional to the edge weight, based on the following logic.  We first calculate the mean and standard deviation of the raw weights. We then assign a new weight, determined by how much the raw weight is below/above the mean. All edge weights equal to or below the mean get a -1 in our new weighting scheme, which will reduce the density of the plot considerably (we set the weight to -1 to ensure that these edges do not show up in the plot). All edges between 0 and 1 standard deviations above the mean weight get a .5, while all edges between 1 and 2 standard deviations get a 1.5. Finally, all edges above 2 standard deviations get a 2.5. Note that these values are only used for plotting purposes. 

```{r}
std_weight <- sd(E(club_net96)$weight)
weight_mean_center <- (E(club_net96)$weight - mean(E(club_net96)$weight))

recode_weight <- E(club_net96)$weight
recode_weight[weight_mean_center <= 0] <- -1
recode_weight[(weight_mean_center > 0) & (weight_mean_center <= std_weight)] <- .5
recode_weight[(weight_mean_center > std_weight) & 
                (weight_mean_center <= std_weight * 2)] <- 1.5
recode_weight[weight_mean_center > std_weight * 2] <- 2.5
```

We will now apply our new weighting scheme to the widths of the edges. We will also change the color of the edges (to be the same across all edges). 

```{r}

E(club_net96)$color <- rgb(.5, .5, 0, .2)
E(club_net96)$width <- recode_weight
V(club_net96)$size <- 3
```

```{r fig.height=7.5, fig.width=7.5}
plot(club_net96, layout = layout_with_lgl)
```

The plot offers important insight into the structure of club membership in the school. We can see that there are a number of grade-specific sports teams on the periphery and a core consisting of more generalist clubs, like Pep Club and National Honor Society (NHS). Now, we want to take the basic intuition from our plot and analyze the network more formally. We will consider key measures of centrality. 

## Club-to-Club Network
Here we will analyze the weighted version of the club-to-club network, based on the idea that some clubs share more members than others. We could, of course, also analyze the binary version of the network (equal to 1 if ij have at least one member in common and 0 if not). The problem with the binary network is that it equates two clubs who have one shared member with clubs who have 5 (6, 7, etc.) common members. Binary networks also tend to be quite dense, as they capture pretty weak relationships in the case of one-mode projections. Many clubs may share at least one member together, making the ties in the network not very differentiating. Thus, we will generally want to use the weighted version of the network when analyzing a one-mode projection of a two-mode network. 

One thing we have to consider is what form the weights should take when doing different calculations. There are a number of different options and different weights are appropriate for different measures. igraph will use the edge weights on the igraph object by default. We can, however, tell igraph, within particular functions, to use a different set of weights, which will be useful in certain cases. Let's go ahead and create a couple different versions of the edge weights (based on the number of shared members). Let's first create a set of standardized edge weights. Here we take the number of members that i and j share but we divide by the standard deviation of those counts, thus standardizing the weights by the spread of the data. Standardizing the weights is useful as the centrality scores can be interpreted on a common metric; convenient, for example, if we are trying to make comparisons across networks.

```{r}
weights <- E(club_net96)$weight 
scaled_weights <- weights / sd(weights) 
```

For certain measures, we will want to use a set of weights based on the inverse of the original weights. Here we take the inverse of the scaled weights calculated above:

```{r}
invscaled_weights <- 1 / scaled_weights
```

### Centrality 
We begin by calculating a simple weighted degree measure. Here we use a `strength()` function. The main arguments are:

- graph = network of interest
- mode = in, out or all (type of degree)
- weights = edge weights to use, by default weight attribute on network

We begin with the default weights, the count of members in common.

```{r}
deg_normalweights <- strength(club_net96, mode = "all") 
```

And here we use the scaled version of the weights.

```{r}
deg_scaledweights <- strength(club_net96, mode = "all", 
                              weights = scaled_weights) 
```

Let's look at the first 10 values for our two centrality calculations:
```{r}
deg_data <- data.frame(deg_normalweights, deg_scaledweights)
```

```{r}
deg_data[1:10, ]
```

Note that if we use the `degree()` function (as we did in other tutorials), this would not use the weights at all, just using a binarized version of the network (where ij = 0 if they have at least one member in common). 

```{r}
deg_noweight <- degree(club_net96, mode = "all")
```

```{r}
cor(deg_scaledweights, deg_noweight) 
```

We can see that the non-weighted version is highly correlated with the weighted calculation but it is not identical. Let's also calculate closeness centrality. The function is `closeness()` (like in [Chapter 9](#ch9-Network-Centrality-Hierarchy-R)) but here we will include an input for weights. For closeness, we will use the inverted version of the weights, as two clubs with more common members are actually closer. Using the regular weights can lead to some odd results, as we demonstrate below. This is the case as higher edge weights mean more members in common, which in turn means lower distance/higher closeness. But in calculating distance (the first step in calculating closeness), igraph treats the weights in the opposite manner, assuming that higher weights imply higher distances between nodes. We thus adjust for that and use the inverted weights. Here we calculate both versions, first using the inverted weights, and then using the regular weights.

```{r}
close_invweights <- closeness(club_net96, weights = invscaled_weights)
close_weights <- closeness(club_net96, weights = scaled_weights)
```

As a quick check let's see how correlated these closeness measures are with degree centrality:

```{r}
cor(deg_normalweights, close_invweights) 
```

Using the inverted weights, closeness and degree are highly correlated, as we would expect. What about the correlation between closeness and degree when we do not use the inverted weights? 

```{r}
cor(deg_normalweights, close_weights) 
```

Using the regular weights, the correlation (between degree and closeness) is lower than what we would expect, suggesting it is problematic to calculate closeness using the regular, not-inverted weighting scheme. 

Now that we have calculated some example centrality scores, let's get a sense of which clubs tend to be most important to this school, in terms of integrating the school socially. Let's see which clubs are the most central for degree, just looking at the top 10. Let's order the centrality scores from high to low and grab the first 10. 

```{r}
deg_top10 <- order(deg_scaledweights, decreasing = T)[1:10] 
```

Now, let's see which clubs correspond to those top 10.

```{r}
toptenclubs_degree <- club_names[deg_top10]
```

```{r}
data.frame(high_degree = toptenclubs_degree)
```

Let's also take a look at the bottom 10 for degree (setting decreasing to F):

```{r}
deg_bottom10 <- order(deg_scaledweights, decreasing = F)[1:10]
bottomtenclubs_degree <- club_names[deg_bottom10]
```

```{r}
data.frame(high_degree = toptenclubs_degree,
           low_degree = bottomtenclubs_degree)
```

The bottom clubs tend to be sports teams and grade (or gender) specific clubs, as opposed to the more generalist clubs making up the high degree clubs. 

Finally, it will be useful to consider which club attributes are associated with network centrality. Here, we will consider club profile, showing how much attention the clubs get in the school. Are higher status clubs more/less central to the network? Let's create a new data frame combining club profile with our calculated centrality scores (after turning `club_profile` into an ordered factor). We will grab `club_profile` from the igraph object.

```{r}
club_profile <- factor(V(club_net96)$club_profile, ordered = T, 
                    levels = c("low", "moderate", "high", "very_high"))

centrality_data <- data.frame(deg_data, club_profile = club_profile)
```

```{r}
head(centrality_data)
```

And now we will use the `aggregate()` function to calculate the mean and standard deviation of degree for each value of `club_profile` (low, moderate, high, very high).

```{r}
aggregate(deg_scaledweights ~ club_profile, data = centrality_data, FUN = mean)

```

```{r}
aggregate(deg_scaledweights  ~ club_profile, data = centrality_data, FUN = sd)
```

The results indicate that lower profile clubs tend to have, on average, higher degree centrality than higher profile clubs. Low profile clubs also have higher variance in degree. This suggests that low profile clubs are quite heterogeneous, with some clubs (like the large, generalist service clubs) central to the network, while others (like low status individual sports) are on the outskirts. Centrality for the low profile clubs is thus driven mostly by the type of club, and this can lead to widely varying results. In contrast, highly visible clubs, like Drill or Varsity Cheerleading, have more uniform positions in the network - rarely the very top, central node, but rarely on the outside of the network either. High profile clubs are high status, in part, because they are exclusive, as it is difficult to gain membership. High profile clubs thus tend to be smaller, with a kind of upper limit to how central (in networks terms) they can be. This might be indicative of a network connectivity versus status/exclusivity trade-off. 

What can we substantively conclude about the organization of this school? First, we can see that central clubs tend to be generalist, serving a potentially wide part of the school. We see things like Pep Club, Spanish Club and National Honor School. These are clubs with relatively low bars for membership and relatively low time commitments. This makes it possible for many people to be members of those clubs, while also simultaneously being part of other clubs in the school. On the other end, we tend to see sports teams and competitive academic teams on the periphery of the club-to-club network. These clubs are a kind of greedy institution, demanding full commitment from its members. This may create internal cohesion, but it also means that sports teams are unlikely to create bridges or to integrate the school as a whole. Members of the tennis team may only see other members of the tennis team.  We also see that high profile clubs tend have lower means and variance for degree centrality, suggesting something about the network consequences of being more exclusive. This analysis has focused on centrality, but it is possible to do other kinds of analyses, such as community detection, blockmodeling and the like. 

### Groups
We will now turn to a short analysis of the group structure of this club-to-club network. The basic question is which clubs tend to cluster together, so that a large number of students tend to be members of the same set of clubs. We will use a simple fast and greedy algorithm here, with the weights included as a key input. In this case, we do not want the inverted version of the weights. A higher number of shared members means the clubs are more likely to be in the same group, indicating a stronger relationship between the clubs. We use the scaled version of the weights.

```{r}
groups_scaledweights <- cluster_fast_greedy(club_net96, 
                                            weights = scaled_weights)
```

Let's see what kinds of clubs are in each group, where a group is defined by a set of clubs that have high (weighted) rates of interaction with each other, defined here by sharing students. To explore this, we will look at the type of clubs that tend to be in each group. We will first put together a little data frame to assist with this, pairing the group memberships with the club name and the type of club (Academic Competition, Team Sports, etc.).

```{r}
group_dat <- data.frame(group = as.numeric(membership(groups_scaledweights)), 
                        name = V(club_net96)$name, 
                        club_type_detailed = V(club_net96)$club_type_detailed)
```

```{r}
head(group_dat)
```
And now let's look at the membership of each group, starting with group 1. 

```{r}
group_dat[group_dat[, "group"] == 1, ]
```

Here we calculate a little table, showing the proportion of clubs (in group 1) that fall into each type of club. 

```{r}
prop.table(table(group_dat$club_type_detailed[group_dat$group == 1]))
```

It looks like group 1 is made up of mostly sport teams. And now for the rest of the groups: 
```{r}
group_dat[group_dat[, "group"] == 2, ]
```

```{r}
prop.table(table(group_dat$club_type_detailed[group_dat$group == 2]))
```

Group 2 is centered around academic clubs and performance art.

```{r}
group_dat[group_dat[, "group"] == 3, ]
```

```{r}
prop.table(table(group_dat$club_type_detailed[group_dat$group == 3]))
```
We can see that group 3 is a small group centered on academic competition.

```{r}
group_dat[group_dat[, "group"] == 4, ]
```

```{r}
prop.table(table(group_dat$club_type_detailed[group_dat$group == 4]))
```

Group 4 is a heterogeneous group, with sport teams, performance art, and generalist service clubs (like key club). 

```{r}
group_dat[group_dat[, "group"] == 5,]
```

```{r}
prop.table(table(group_dat$club_type_detailed[group_dat$group == 5]))
```

 Finally, looking at group 5, we see a small group centered around sports teams (specifically for younger students). Now, let's get a broader picture of the group structure. Let's plot the network but color the nodes based on group membership. 

```{r}
V(club_net96)$color <- membership(groups_scaledweights)
```

Let's also highlight those nodes who have high degree. This will let us see how the central nodes fall into different groups. We will color the frame of the top ten nodes red, as a means of highlighting those high degree clubs.

```{r}
V(club_net96)$frame.color[deg_top10] <- rgb(1, 0, 0, .75)
```

Let's plot the network, and use the group ids as the labels.

```{r fig.height=7.5, fig.width=9.5}
set.seed(105)
group_label <- group_dat$group
plot(club_net96, main = "Coloring by Groups", 
     vertex.label = group_label, vertex.label.cex = .75)
```

Note that a few of these clubs don't seem to be placed very well. For example, JV volleyball is quite isolated and difficult to place in a group (the isolated green node). We may consider breaking them out into their own group. Assuming we are satisfied with the analysis, we can proceed to interpret the found groups. The plot makes clear that there are two periphery groups (group 1 and group 5) that consist of sports teams that have low co-membership with other clubs. The remaining groups (2, 3, 4) are more heterogeneous and make up the core of the network, with lots of co-membership across clubs in different groups. We also see that the most central clubs are spread across those three groups. So, it is not the case that all of the most central clubs fall into a single group. Instead, the different groups revolve around different clubs that help integrate that set of students. For example, group 2 is centered around NHS, Spanish club high and French Club high. Group 4 is centered around Spanish Club (low), French Club (low) and Pep club. Roughly speaking then, both group 2 and group 4 are integrative, heterogeneous groups, but one centers more strongly on academic excellence than the other.  We thus have a set of membership patterns that generally fall into: exclusively sports (group 1 and 5), sports, service and performance art  (group 4), academic competition (group 3) and mixed academic (group 2).

## Using the tnet package
We have thus far analyzed our weighted networks by treating the edges as valued, and summing up those valued edges when calculating things like degree centrality. The downside of this approach is that it does not differentiate the weights on the edges from the number of edges. For example, an actor with one very strong relationship could have high weighted degree, perhaps higher than other actors with many (somewhat weaker) relationships. On the other hand, ignoring the weights only captures the number of edges, treating each edge as a 0 or a 1. We can draw on the **tnet** package to calculate measures that get beyond some of these limitations (@tnet). The basic idea is to include a parameter that puts a relative weight on the number of edges compared to the edge weights (strength of relationship). Let's first read in the **tnet** package. 

```{r message=F, warning=F}
library(tnet)
```

And now we need to get the data in a form that **tnet** can use. The **tnet** package takes a weighted edgelist, with the last column capturing the weights. Let's get the edgelist from the igraph object. 

```{r}
club_edgelist96 <- as_edgelist(graph = club_net96, names = F)
```

And now we add the scaled weights to the edgelist and put useful columns names onto the matrix.

```{r}
club_edgelist96 <- cbind(club_edgelist96, scaled_weights)
colnames(club_edgelist96) <- c("sender", "receiver", "weights")
```

The **tnet** package doesn't handle undirected networks especially well. So, before we run any analysis, we need to construct the edgelist so that every edge from i to j has the reverse edge also included (j to i). We will accomplish this by taking the edgelist, switching column 1 and column 2, and then stacking it underneath the current edgelist. 

```{r}
club_edgelist96 <- rbind(club_edgelist96, club_edgelist96[, c(2, 1, 3)])
```

```{r}
head(club_edgelist96)
dim(club_edgelist96)
```

We can see that there are 2722 edges (or 1361 undirected edges). Now, we are ready to calculate our weighted centrality measures. For degree, the function is `degree_w()`. The main arguments are:

- net = network of interest, as weighted edgelist
- measure = type of measure: degree calculates measure ignoring the weights; alpha considers the weights
- type = out or in degree
- alpha = weight to put on counts compared to edge weights. 0 = all weight on counts; 1 = all weight on edge weights

We will first calculate degree putting all of the weight on the edge weights, setting alpha to 1. This means that the number of edges is ignored in the calculation, and we simply sum up all of the weights (i.e., the weights for each edge involving node i). 

```{r}
degree_alpha1 <- degree_w(net = club_edgelist96, measure = "alpha",
                          type = "out", alpha = 1)
```

```{r}
head(degree_alpha1) 
```

The output is a matrix with two columns, one with the ids and one with the centrality score (the `alpha` column). This is the same as the calculation in **igraph** using the weights to calculate degree. We stored those results in `deg_scaledweights`. Now, let's do the other extreme and put all the weight on the edge count, ignoring the edge weights (alpha = 0). 

```{r}
degree_alpha0 <- degree_w(net = club_edgelist96, measure = "alpha",
                          type = "out", alpha = 0)
```

```{r}
head(degree_alpha0)
```

This is the same as the calculation in **igraph** that ignores the weights, stored in `deg_noweight`. Finally, let's do something in between and set alpha to .5, putting some weight on both the counts and edge weights. 

```{r}
degree_alpha.5 <- degree_w(net = club_edgelist96, measure="alpha", 
                           type = "out", alpha = .5)
```

We can check to see how much this affects the results by calculating the correlation between our measures. We need to take the second column from the output, as this is the centrality measure of interest.

```{r}
cor(degree_alpha1[, 2], degree_alpha.5[, 2])
cor(degree_alpha0[, 2], degree_alpha.5[, 2]) 
```

We can see that in this case setting alpha to .5 offers very similar results to having full or no weight on the edge weights. 

We now turn to the same exercise for closeness. Here the function is `closeness_w()`. The main arguments are again net and alpha.  We start by setting alpha to 1, putting full weight on the edge weights. Note that in this case we do not have to invert the weights (as we did in **igraph**) as the algorithm employed here does this by default. 

```{r}
close_alpha1 <- closeness_w(net = club_edgelist96, alpha = 1) 
```

```{r}
head(close_alpha1) 
```

The output is a matrix with three columns: ids, closeness and scaled closeness (closeness / (n - 1)). This is directly analogous (although there are some slight technical differences) to our calculation in **igraph** using the inverted weights and the `closeness()` function. This was stored as `close_invweights`. And now we can do the same thing setting alpha to 0 (ignoring edge weights). 

```{r}
close_alpha0 <- closeness_w(club_edgelist96, alpha = 0) 
```

```{r}
head(close_alpha0) 
```

To get the same calculation in **igraph** we would do the following. 

```{r}
close_igraph_noweight <- closeness(club_net96, weights = NA)
```

Note that weights is set to NA, as the default is to use the weights on the igraph object and we don’t want that here. 

```{r}
head(close_igraph_noweight)
```

We can see it is the same as the second column above. And again we can set alpha to a value different than 0 or 1. 

```{r}
close_alpha.5 <- closeness_w(club_edgelist96, alpha = .5) 
```

Let's check the correlation between putting no weight on the edge weights (alpha = 0) and .5 weight:

```{r}
cor(close_alpha0[, 2], close_alpha.5[, 2]) 
```

We see the versions of closeness centrality are highly correlated but not identical.

## Racial Segregation
In this section, we shift our focus to the attributes of the students and how this maps onto affiliation patterns. Our main question is how strong homophily is along racial lines. Do students tend to be members of clubs with students of the same racial identity? Or is there racial heterogeneity in club memberships? There are a number of ways that we might address this question, and here we take two different approaches. In the first case, we ask how many (weighted) edges connecting students are homophilous. If student i and student j are both members of Spanish Club, are they the same or different racial identity? We will calculate this over all co-membership edges (where student i and student j are in the same club) and then ask what proportion of all (weighted) edges match on race. In the second case, we will calculate a slightly different version of the same homophily statistic; we will calculate how many times a student is in a club with at least one other student of the same race. This captures a kind of baseline level of homophily: do students ever join clubs where they cannot find a student of the same race? 

Note that these calculations set the stage for our analysis in [Chapter 13](#ch13-Two-mode-Network-Models-ERGM-STERGM-R), where we cover statistical models for two-mode networks. The two versions of racial homophily represent two (extreme) versions of the kinds of homophily terms that can be included in our statistical models.

### Proportion of Co-membership Edges where Students Match on Race
In this section we walk through the steps to calculate the number of weighted edges that match on race. We begin by grabbing the student-student network from the one mode object constructed above (here we need `proj1`, rather than `proj2`).

```{r}
student_net96 <- onemode96$proj1
```

It will be useful to work with the edgelist version of the network. Here we extract the edgelist from the student-student network, adding the weight column to the data frame. 

```{r}
student_edges <- as_edgelist(graph = student_net96)
student_edges <- data.frame(student_edges, weight = E(student_net96)$weight)
colnames(student_edges)[1:2] <- c("sender", "receiver")
```

And let's take a look at the edgelist: 
```{r}
head(student_edges)
```
These edges capture the co-membership ties between students. So, for example, student 101498 and 113060 are in one club together, while 101498 and 114037 are in three together (looking at the weight column). We can also think of these weighted edges as the number of two-paths between student i and j; as i and j are connected in the original, two-mode network through a common club. For example,  101498 and 113060 are both members of NHS, creating a two-path in the two-mode network (101498-NHS-113060) or one edge in the one-mode network (101498-113060). It will be useful to consider the two-path interpretation as the statistical models covered in [Chapter 13, Part 3](#ch13-Two-mode-Network-Models-ERGM-STERGM-R) will specify these kinds of processes using the two-mode specification. 

In order to calculate if i and j match on race (assuming they are in the same club), we need to get the racial information in a more accessible format. Here we will grab the ids of the students and their racial information and put them together in a data frame.

```{r}
race_dat <- data.frame(name = V(student_net96)$name, 
                       race = V(student_net96)$race)
```

```{r}
head(race_dat)
```

And now we can go ahead and map the racial information of the nodes onto the edgelist extracted above. The goal is to get the race of the sender and receiver of each edge onto a single data frame, to facilitate our homophily calculation (do i and j identify as the same race?). We will use a `match()` function to accomplish this. The idea is to match the id from `race_dat` (based on name) with the sender (or receiver) id from `student_edges`. We will then grab the race value associated with that sender (or receiver) id.  We will do this once for sender and once for receiver, and then put everything together in a data frame. 

```{r}
sender_race <- race_dat[match(student_edges[, "sender"], 
                              race_dat[, "name"]), "race"]

receiver_race <- race_dat[match(student_edges[, "receiver"], 
                                race_dat[, "name"]), "race"]

student_edges <- data.frame(student_edges, sender_race, receiver_race)
```

```{r}
head(student_edges)
```
We now have the sender id, the receiver id, the weight on the edge, as well as the sender and receiver racial identity. Let's see which of those sender-receiver pairs is the same race:

```{r}
same_race <- student_edges[, "sender_race"] == student_edges[, "receiver_race"]
```

We are now in a position to calculate our statistic of interest, the proportion of weighted edges between students where student i and j are of the same race (or the number of two-paths that match on race). We need two pieces of information: first, the total number of weighted edges that match on race; and second, the total number of weighted edges. The total number of weighted edges can be calculated by summing the weight column in our `student_edges` data frame. The number of weighted edges that match on race is based on the same calculation but restricted to where `sender_race` is the same as `receiver_race`, identified above.

```{r}
sum(student_edges[same_race == T, "weight"]) / sum(student_edges[, "weight"])
```

This means that about .468 of all connections between i and j match on race. This suggests that many co-membership ties exist between students of different racial identities, consistent with the figure constructed above. Note that this result might be partly driven by generalist clubs (like NHS) which tend to be large, racially heterogeneous entities. Such clubs would have a disproportionate impact on increasing the number of two-paths that mismatch on race. We will consider this more carefully in [Chapter 13, Part 3](#ch13-Two-mode-Network-Models-ERGM-STERGM-R), where we will examine racial homophily, while controlling for other factors, like differences in club size. 

### Do Students Match Racially with at Least One Other Student? 
We now turn to our second set of analyses, where we ask how often students match, racially, with at least one other student in the club (they are a member of). Here it will be easier to work with the two-mode version of the network. We will construct an edgelist with the sender as the students and the receiver as the clubs. We set names to F, as we do not want the node labels, instead outputting the node number in the edgelist (this will make it  easier to map race onto the sender/receiver for that edge).

```{r}
club_student_edges <- as_edgelist(graph = affil_net96, names = F)
colnames(club_student_edges)[1:2] <- c("sender", "receiver")
```

```{r}
head(club_student_edges)
```

And now we will grab the race attributes of the nodes from the igraph object (the two-mode version of the network): 

```{r}
race2 <- V(affil_net96)$race
```

We now need to determine for each edge, student-club, if the student is the same race as at least one other member of the club. There are a number of ways we could go about calculating this. Here we will lay out each step in a loop (which might be a slow option for a large network). The basic logic is to first grab the race of the focal student (in the edge in question). We then grab the race of all other students in that club (excluding the student of interest). We then ask if the race of the focal student matches any student in the club. This is done over all student-club edges in the network and stored in an object called `match_race`.

```{r}
match_race <- NA

for (i in 1:nrow(club_student_edges)){
  
  # grabbing race of student in edge:
  send_race <- race2[club_student_edges[i, "sender"]]
  
  # getting race of students in club of interest, excluding student in edge:
  club_id <- club_student_edges[i, "receiver"] 
  edges_to_club <- club_student_edges[-i, "receiver"] == club_id
  student_ids <- club_student_edges[-i, ][edges_to_club, "sender"] 
  club_race_composition <- race2[student_ids] 
  
  # asking if race of focal student race matches any other student in club:
  match_race[i] <- send_race %in% club_race_composition
  }

```

```{r}
head(match_race)
```
This tells us, for example, that for the first edge, the student (101498) is the same race as at least one other student in the club (NHS). And now we can calculate the overall proportion that match, calculated over all student-club edges in the network:

```{r}
prop.table(table(match_race))
```

This suggests that almost all students have at least one other student in their clubs that matches on racial identity. In this way, students are very unlikely to be the sole, 'outsider' differing on race with all other members of the club.  This is particularly interesting in the case of smaller racial groups, like Native American or Asian, where, by chance, we might expect them to join clubs and be the only other person of their racial identity. The fact that this rarely happens is telling. 

Substantively then, our two analyses suggest that students are likely to be in clubs where some of the students are of a different race (based on the first calculation), but are very much unlikely to find themselves racially isolated in a club (based on the second calculation). In this way, students are unlikely to join clubs where there is a bad mismatch between their race and the race of the other students; but it does mean that a club can have a mix of different racial groups. Of course, some of the racial matching might arise by chance and it is important to determine how much of the racial homophily is driven solely by the overall composition of the school (as well as other factors of interest, like the tendency for students to join similar types of clubs). We return to this in [Chapter 13, Part 3](#ch13-Two-mode-Network-Models-ERGM-STERGM-R), statistical models for two-mode networks.  

## Club Centrality and Attributes
As a final example, we connect the analysis of club centrality to the analysis of student attribute segregation. In Section 4.1 we determined which clubs were more/less central to the network. Here, we reexamine this question, while incorporating information about the student attributes (see Section 11.6.1 and 11.6.2 above). The basic question is whether more central clubs tend be more or less segregated in terms of student attributes, like grade, gender or race. Are more peripheral clubs more homogeneous? And more formally, is the proportion of students with the same grade (or gender) higher in central, bridging clubs or in peripheral, exclusive ones? For example, we might think that the proportion matching would be higher (so more segregated) in the periphery, harder to access clubs. If so, that would suggest that central clubs are locales for multiple grades and genders, acting as integrating spaces across key student identities. 

Our goal is to calculate the proportion of student pairs within each club that have the same gender or grade. We will then correlate the proportion matching with the centrality scores previously calculated. 

As a first step, we need to determine which students are in which clubs. To accomplish this, we will create a list, where each element shows the student ids in the club in question. We will work with the raw affiliation matrix, `affiliations96_nomiss`. For each column in the matrix (i.e., each club), we will identify which students have a "1", and are thus members of the club. We will do this for all clubs using `apply()` and setting MARGIN to 2 (in order to repeat the function for each column). We set the function of interest using the FUN input.

```{r}
student_id_list <- apply(affiliations96_nomiss, MARGIN = 2, 
                         FUN = function(x) which(x == 1))
```

The output is a list, one entry (of student ids) for each club. Let's look at the first two clubs: 

```{r}
student_id_list[1:2]
```

We see the students who are in the Academic decathlon club and the Art Club. The numbers correspond to the rows in the affiliation matrix for the student in question.

And now we need to write a little function to calculate the proportion of students who match on grade or gender in a given club. This is a little different than what we accomplished with the racial segregation analysis above (11.6.2), where we asked if a particular student in a club matched or not with other students in the club. Here, we ask what proportion of all student pairs in a club match on the attribute of interest: do student i and student j (over all i,j in the club) have the same grade, gender, etc.?  

Our function will take two arguments: first, a vector of student attributes (grade or gender) for all students in the school; and second, the ids of the students in the club of interest. Note that the input ids are not based on the labels, but refer to the rows in the affiliation matrix, as stored in `student_id_list`.

```{r}
prop_same_func <- function(attribute, ids){

  #Arguments:
  #attribute: vector of attributes
  #ids: ids of nodes
  
  # subset attributes to just students in club of interest
  club_subset <- attribute[ids]
  
  # calculate number in each category
  tab_attribute <- table(club_subset)

  # calculate number of students in club
  num_students <- sum(tab_attribute)

  # calculate number of pairs that match; accomplished
  # by squaring and summing the number in each category
  # while also adjusting for diagonal, 
  # as we do not want to count the number of
  # times i matches with self
  num_match <- sum(tab_attribute ^ 2) - num_students

  # now calculating proportion that match, by dividing
  # number that match by total number of pairs, excluding 
  # diagonal (where i is the same student)
  prop_same <- num_match / (num_students * (num_students - 1))
}

```

And now we will apply our function to the vector of gender values for our students. We will first grab gender from the igraph object (just for students, so setting type to FALSE). We then apply our function over each set of students ids, one for each club; this is stored in `student_id_list`. We use `lapply()` as we want to run the function over each element of `student_id_list`. 

```{r}
gender <- V(affil_net96)$gender[V(affil_net96)$type == FALSE]

same_gender_prop <- lapply(student_id_list, FUN = prop_same_func, 
                           attribute = gender)
```

It will be useful to have the calculated values stored as a numeric object (rather than a list), so let's go ahead and do that using an `unlist()` function. 

```{r}
same_gender_prop <- unlist(same_gender_prop)
```

```{r}
head(same_gender_prop)
```

And we can see that we have a numeric vector showing the proportion of student pairs in the club that match on gender. So, for example, .488 of all pairs in Academic decathlon are the same gender. 

And now we can go ahead and do a simple correlation analysis. We will correlate the centrality scores, based on degree centrality, with our same gender proportion measure. 

```{r}
cor(deg_normalweights, same_gender_prop)
```

And now we do the same thing for grade:
```{r}
grade <- V(affil_net96)$grade96[V(affil_net96)$type == FALSE]

same_grade_prop <- lapply(student_id_list, FUN = prop_same_func, 
                          attribute = grade)

same_grade_prop <- unlist(same_grade_prop)

cor(deg_normalweights, same_grade_prop)
```

It looks like less central clubs, do in fact, have higher levels of segregation, being more homogeneous in terms of gender and grade (especially for gender). We see a negative correlation, so that more central clubs have lower proportion matching. This suggests that the affiliation network is held together by low commitment ('blow-off') clubs that anyone can join and this serves to bring disparate group interests (e.g., on gender, on age and skill) into contact with one another, even if fleeting. The more demanding, distinguishing activities play to core identities and are more exclusive. As such, the affiliation network reinforces identity-selection and differentiation but provides means for secondary association and integration across those interests via easy to access clubs. In this way, intermittent locales arise to maintain overall integration.

This lab on two-mode networks has included a wide variety of analyses, opening up important questions about the memberships that individual students take on. We could push this analysis further by considering these processes together, in a single model. For example, how much does the composition of the clubs (gender, race, etc.) matter when students decide which clubs to join? Or is it simply about the clubs type (sports, academic, etc.)? The larger goal would be to tease out the decision rules that student use in joining clubs. See [Chapter 13](#ch13-Two-mode-Network-Models-ERGM-STERGM-R) for examples. Further analyses could also look more closely at the kinds of trajectories that students take on and the consequences for such different career paths within the school.

This tutorial has covered the analysis of two-mode data. In [Chapter 12](#ch12-Networks-Culture-R), we will take up related issues of duality when analyzing cultural (or textual) data. We will cover topic modeling and latent spaces. 


# 12, Part 1. Cultural Structures {.unnumbered #ch12-Networks-Structure-Culture-text-R}

This is the first tutorial for Chapter 12, covering the application of network ideas to the analysis of cultural data. This tutorial will build directly on the material from Chapter 11, on two-mode data. In [Chapter 11](#ch11-Two-mode-Networks), we analyzed typical affiliation data, where the rows were actors and the columns organizations. In this tutorial, we will be working with textual data, applying the ideas of duality to a very different kind of data source, one based on words in documents. We will walk through the basics of topic modeling in R. Topic modeling is a natural language processing technique that uncovers the latent topics that structure the words used in a set of documents.  We will cover the following: basic data management of textual data; modeling textual data using LDA (latent Dirichlet allocation); and representing textual data as a network.

For our empirical case, we analyze textual data based on a set of sociology abstracts (drawn from recent dissertations). We are interested in discovering the latent topics that exist in the data, where each topic is defined by having a distinct pattern of words associated with it. We are also interested in seeing which abstracts get placed together and why. In this way we are trying to uncover the underlying structure of the field of sociology (as represented in abstracts), where certain words and researchers are associated with a topic and certain topics are closer to each other than others. We thus see the intuition of a network approach played out using textual data. As this is a textbook on network analysis, we will not focus on the technical details of topic modeling; instead, we will focus on the substantive application of network ideas to textual data. For those interested in more technical details on topic models, see the **topicmodels** [documentation](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf). And for information on text mining and textual analysis in general, see @Feinerer2008.

## Getting the Data Ready
We will need a number of packages to analyze our textual data, here dissertation abstracts in sociology. We will need: **NLP** (basic processing of natural language), **tm** (text mining package), **SnowballC** (word stemming package), **topicmodels** (package for modeling the topics), and **ldatuning** (package for evaluating the models). Let's go ahead and load them all.

```{r message=F, warning=F}
library(NLP)
library(tm)
library(SnowballC)
library(topicmodels) 
library(ldatuning) 
```

Note that topic modeling is computationally heavy and can require a long run time. The data we will use for this lab is only a very small (random) sample of the original corpus of dissertations. If this was an analysis for an actual paper, we would want to use the whole set of data, or at least a much larger sample.  Let's read in the data, treating our text strings as strings rather than as categories:

```{r}
url1 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/sociologysample.csv"

abstracts <- read.csv(file = url1, stringsAsFactors = FALSE)
```

We can use the `str()` function to take a look at the data.

```{r}
str(abstracts) 
```

We see there are two columns (`obs` and `text`), the first for the observation and the second for the actual text of the abstract. There are 199 abstracts in the data. For example, let’s look at the text of the first abstract. 

```{r}
abstracts[1, "text"] 
```

We can see that this is literally the text of the dissertation abstract, word for word. This is the data that we want to analyze. Before we can analyze the data, we need to create a corpus based on the words used in the abstracts. We basically need to transform the raw text data into a set of words (for each abstract) that are directly comparable across abstracts. We want to know which words are used together frequently and which abstracts are using which words. We thus need to have the data in a format where such comparisons are possible. By creating a Corpus object from our abstracts, we will be able to use various functions in R that are designed to standardize text data.

Here we will use a `Corpus()` function. Note that the `Corpus()` function will not take text data directly as input. We thus need to use a `VectorSource()` function within the main function, with the input as the text of interest (`abstracts$text`).

```{r message=F, warning=F}
abstracts_corp <- Corpus(VectorSource(abstracts$text))
```

```{r}
length(abstracts_corp)
```

The length is 199, one for each of the abstracts in our original data. It will be useful to clean the corpus a bit before we actually try to analyze it. Textual data can be messy. We need to make the inputs (abstracts in this case) as uniform as possible, to facilitate comparison. We will go through a series of steps on how to streamline the textual data, making the abstracts consistent and informative. For example, let's change everything to lower case. If one dissertation uses the word "network" and another uses "Network", we don’t want to treat them as having used different words. We will make use of the `tm_map()` function to clean up the corpus. The inputs are the corpus and the function we want to use (telling R how to change the text). Here we will set the function to tolower.

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, tolower)
```

Let's take a look at the first abstract:

```{r}
abstracts_corp[[1]]$content
```

We can see all the words are now lower case. We also need to deal with some odd mistakes in the data. We can see from the first abstract that the raw textual data sometimes has multiple words stuck together. For example, in the first abstract current and depression are put together as current_depression. That does not yield a meaningful word, so we need to split those words up into separate words, whenever that happens. In this case, we need to write our own little function (called `split_words()`) to perform this task. This function will make use of `content_transformer()`, which is a function used to create text transformations that are not already built into R. Here, we will write a little function within `content_transformer()` that will find all the _ and replace them with a space. This is accomplished using the `gsub()` function. The main arguments to `gsub()` are pattern (the pattern to look for) and replacement (the replacement text for the specified pattern). x is the text of interest.  

```{r}
gsub_function <- function(x, pattern) gsub(pattern, replacement = " ", x)
split_words <- content_transformer(gsub_function)
```

And here we apply our function to the corpus using `tm_map()`.

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, split_words, pattern = "_")

```

Let’s check to make sure it worked, looking at the first abstract:

```{r}
abstracts_corp[[1]]$content 
```

It looks right. We can see that current and depression are now separate words, rather than current_depression. Let’s do a similar splitting based on words put together with a /.

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, split_words, pattern = "/")
```

Let's also remove punctuation as this is not substantively useful. The function here is `removePunctation()`. This will take out commas, periods, apostrophes and so on.  In this case we do not need to use a content_transformer function as `removePunctuation()` is a built-in transformation in the **tm** package. 

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, removePunctuation)
```

Let's also remove all of the numbers from the corpus.

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, removeNumbers)
```

Now let's designate a set of words to remove. For example, we may want to remove commonly used words that don't add much to differentiate word use in the abstracts. For example, if everyone uses "is" it may make sense to remove it from all abstracts. Here we can use the `stopwords()` function (set to english) to get commonly used words. 

```{r}
stopwords("english")
```

Here we remove any word in that list from each abstract using the `removeWords()` function.

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, removeWords, stopwords("english"))
```

Now, let's add a few more words to our stopword list. Again, we want to remove words that are not differentiating for the corpus at hand (here abstracts from sociology dissertations). We will add the following words that were not in the default stop list. 

```{r}
myStopwords <- c("dissertation", "chapter", "chapters", "research", 
                 "researcher" ,"researchers" ,"study", "studies", 
                 "studied", "studys", "studying", "one", "two", "three")
```

We tried to include words that are not informative for this corpus, as they are generic and widely used. For example, many abstracts may mention a "chapter one" but that does not make it substantively important that chapter and one are used together frequently.  We could, of course, imagine a slightly different list, and we must be aware that are our results will differ if we exclude different words. We now go ahead and remove those words from the corpus. 

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, removeWords, myStopwords)
```

Now, let's take out any redundant whitespace between words using the `stripWhitespace()` function. 

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, stripWhitespace)
```

Finally, we will stem the document using a `stemDocument()` function. This reduces similar words to a single stem word. For example, test and testing would be reduced to test. The idea is that they convey the same basic meaning and should be treated as the same. 

```{r message=F, warning=F}
abstracts_corp <- tm_map(abstracts_corp, stemDocument)
```

Let's again take a look at the first abstract: 

```{r}
abstracts_corp[[1]]$content
```

After all of this you end up with a string of word stems with many common words removed. This is done for each input (here abstract) in the corpus. Note that the cleaning process is fraught with difficulties. For example, stemming is far from perfect and can lead to some unexpected results. For example, experiment and experience have the same stem (experi) while child and children do not. This could be what you want, but it may not. More generally, synonyms will not be treated as the same word (i.e., stemming will not capture the idea that kid and child are often used to capture similar ideas). Similarly, we have to be careful about the choice of words to include in the stop list. We want to remove words that are not differentiating, but how long of a list we should construct and which words should be included are difficult (context-specific) questions. 

Assuming we are satisfied with the cleaning process, we are now in a position to create a document-term matrix. We need to create a document-term matrix as this will serve as input to our LDA model, where the words and abstracts are placed into latent topics. The document-term matrix is a complex object, capturing how many times each document (the rows) used a particular term (the columns). We will use a `DocumentTermMatrix()` function, with the corpus as input.  

```{r}
abstracts_dtm <- DocumentTermMatrix(abstracts_corp)
```

```{r}
abstracts_dtm
```

We can see that there are 199 documents and 4298 unique terms in our document-term matrix. The values in the matrix correspond to the number of times each document (i.e., abstract) used a given term (i.e., word). 20228 of the values in the matrix are non-sparse, or greater than 1, and 835074 are sparse, equal to 0; where a 0 means that the document did not use the term.  Thus, about 98% of the possible 'ties' between documents and terms do not actually exist (`835074 / (20228 + 835074)`), suggesting that many words are not used widely across abstracts. We also see that the longest word in the corpus is length 25 (i.e., 25 letters).

We can use the `inspect()` function to take a look at particular documents or terms. Here we look at the first abstract:

```{r}
inspect(abstracts_dtm[1, ]) 
```

By default, inspect will print the top ten terms used. We can see here that the first document used the word 'abus' 14 times, 'child' 5 times, and so on.  We can also use inspect to look at the columns. Here we look at the top ten documents that use the term risk:

```{r}
inspect(abstracts_dtm[, "risk"]) 
```

We can see that the term 'risk' is used 9 times in the first document, 1 time in the 116th document and so on. 

We can also use the `Terms()` function to get all of the terms (i.e., words) used and the `Docs()` function to get all of the document ids (here abstracts). Here we look at the first 6 terms.

```{r}
head(Terms(abstracts_dtm))
```

Finally, it may be of use to extract the actual matrix from the document-term matrix. Here we apply `as.matrix()` on the document-term matrix:

```{r}
mat_abstract_words <- as.matrix(abstracts_dtm)
```

```{r}
dim(mat_abstract_words)
```

We can see there are 199 rows (documents) and 4298 columns (terms).  And let's take a look at the first five rows and columns:

```{r}
mat_abstract_words[1:5, 1:5]
```

We can see, for example, that the first document used 'abus' 14 times (same as we saw above). We can take this matrix and calculate summary measures. For example, we can calculate how many different terms the first document used (by asking how many times the first row of the matrix is greater than 0):

```{r}
sum(mat_abstract_words[1, ] > 0)
```

Note that the document-term matrix is analogous to the affiliation matrices we saw in the previous tutorial, where students were on the rows and clubs were on the columns. Here, with textual data, we have documents on the rows and terms on the columns.

## Topic Modeling 
Now, we want to analyze our document-term matrix, applying topic models to the text-based data. We will utilize LDA, latent Dirichlet allocation. LDA attempts to uncover the underlying, or latent, topics in the corpus of interest. Different (latent) topics create different word use and we can use the co-occurrence of words in a document to uncover which words hang together under a given topic. A topic will have a high probability of yielding a set of words when those words are used together at high rates. In a similar way, we can ask which abstracts are likely to fall into which topic, based on their distribution of word choice. In many ways, this is conceptually similar to the positional analysis from [Chapter 10](#ch10-Positions-Roles-R), where nodes were placed in the same position if they had the same pattern of ties to other nodes. Here, two abstracts are likely to be in the same topic if they use the same set of words. 

### LDA: Initial Model
We need to set a few parameters before we can run the model. LDA utilizes Gibbs sampling, a randomized algorithm for obtaining a sequence of observations from a multivariate probability distribution. This sequence will be used to approximate the joint distribution of topics and words. Here we set the key inputs to the algorithm: 

```{r}
burnin <- 200 # number of omitted Gibbs iterations at beginning
iter <- 3000 # number of iterations
thin <- 2000 # number of omitted iterations between each kept iteration 
seed <- list(2003, 5,63, 100001, 765) #seeds to enable reproducibility
nstart <- 5 # number of repeated random starts
best <- TRUE # only continue model on the best model
```

The model also requires that a researcher set the number of topics prior to estimation (similar to setting the number of clusters in [Chapter 10](#ch10-Positions-Roles-R)). Here we will set the number at 5, noting that this is a pretty arbitrary choice. We will consider more principled ways of setting the number of topics below. 

```{r}
k <- 5
```

Now we are ready to run LDA using Gibbs sampling. The function is `LDA()`. The main arguments are:

- x = document term matrix
- k = number of topics
- method = either VEM or Gibbs
- control = list of control input 

```{r}
ldaOut <- LDA(x = abstracts_dtm, k = k, method = "Gibbs", 
              control=list(nstart = nstart, seed = seed, best = best,
                           burnin = burnin, iter = iter, thin = thin))
```

Now let's explore the results. First, we can use a `topics()` function to extract the most likely topic for each abstract. 

```{r}
ldaOut_topics <- topics(ldaOut)
```

```{r}
head(ldaOut_topics)
```

This suggests that the first abstract is most likely to fall into topic 1, the second is likely to fall into topic 2 and so on. We can also extract more nuanced information, looking at the probabilities of each abstract going into each latent topic. Thus, rather than just looking at the most likely topic, we can see the relative probability of a given abstract belonging to a given topic. The probabilities can be extracted using `@gamma` on the model object:

```{r}
topicProbabilities <- ldaOut@gamma
```

```{r}
head(topicProbabilities)
```

We can see that the first abstract has a `r round(topicProbabilities[1, 1], 3) ` probability of being in topic 1 and much lower probabilities for the other topics. Of course, at this point we do not know anything about what the latent topics correspond to. So, let's go ahead and take a look at the topics. Here we will look at the most likely words associated with each latent topic. This amounts to finding the probability that a given topic will yield a given word and selecting those words with the highest probability. We use the `terms()` function with the lda object as input, as well as the number of terms to consider (here set to 10). 

```{r}
ldaOut_terms <- terms(ldaOut, 10)
```

```{r}
ldaOut_terms
```

We can see that topic 4 (for example) yields words like women, student and education, while topic 1 yields words like parent, family, and children. Note that we can get the actual probabilities for the terms using the `posterior()` function. More generally, it is important to see that the topics are simultaneously constituted by the set of words that are used together and the abstracts that tend to use those words, analogous to the ideas explored in the tutorial on two-mode, affiliation networks.  

### LDA: Picking the Number of Topics
Our initial analysis set the number of latent topics to 5. But we could just as easily have used a different number, yielding a different set of results. The issue of how to determine the appropriate numbers of topics is a difficult one, in part, because there is no single, universally accepted way of doing this. One approach is to redo the analysis using different numbers of topics, interpreting the latent topics at different scales, or resolutions. A quick overview of a corpus may require 10 topics, while something more refined may require 25. We may need 100 topics for a really detailed topical inspection. This strategy is fairly robust (as one does not have to pick a single number of topics) but is also somewhat unsatisfying. It would be nice to have a simple set of results that we can point to as our best estimate of what the true latent topics are. For this, we need a systematic way of finding the optimal number of topics for the corpus. Here we can make use of the functions in the ldatuning package. The basic idea is to rerun the model using increasingly higher number of topics. As k increases, we calculate different fit statistics as a means of comparison, eventually picking the one with the best fit. There are a number of possible fit statistics we could use. Here we utilize two, one suggested by @Arun2010 and the other by @Juan2009. For example, @Arun2010 suggest selecting the model (i.e., number of latent topics) that minimizes the cosine distance between topics. The main function is `FindTopicNumber()`. The main arguments are:

- dtm = document-term matrix
- topics = vector of number of topics to consider
- metrics = which metrics to use to evaluate different topic numbers
- method = estimation routine, same as with LDA
- control = list of controls, same as with LDA
- mc.cores = number of cores to utilize

Here, we rerun the model, varying the number of topics from 4 to 40 (doing it every other number to save time). We set the metrics to "CaoJuan2009" and "Arun2010". Note that this can take a bit to run. 

```{r message=F, results='hide', warning=F}
fitmodel <- FindTopicsNumber(dtm = abstracts_dtm, 
                             topics = seq(from = 4, to = 40, by = 2), 
                             metrics = c("CaoJuan2009", "Arun2010"), 
                             method = "Gibbs", 
                             control = list(nstart = 1, seed = c(30), 
                                            best = best, burnin = burnin, 
                                            iter = iter, thin = thin), 
                             mc.cores = 2,  verbose = TRUE)
```

```{r message=F}
fitmodel
```

Once we fit the model under different numbers of topics, we can go ahead and plot the fit statistics. The function is `FindTopicsNumber_plot()`.

```{r message=F, warning=F}
FindTopicsNumber_plot(fitmodel) 
```

Based on our two fit statistics of interest, we might choose a number of topics around 30. The Arun statistic is technically minimized at 40, but given that the CaoJaun fit statistic gets worse after 30 (and the Arun statistic has basically plateaued at this point), we may be in better shape using k at 30. If we were interested in examining a less disaggregated solution, setting k to 10 or 16 would be appropriate. We will now go ahead and rerun the same model as before (with the original control parameters) but set k to 30.

```{r}
k <- 30

ldaOut2 <- LDA(x = abstracts_dtm, k = k, method = "Gibbs", 
              control = list(nstart = nstart, seed = seed, best = best,
                             burnin = burnin, iter = iter, thin = thin))
```

And now let's look at the results. We will focus on the terms, showing the most likely terms for each topic (i.e., the terms that each topic is most likely to yield). 

```{r}
ldaOut_terms2 <- terms(ldaOut2, 10)
```

Let's look at a few example columns (out of the 30 total): 

```{r}
ldaOut_terms2[, c(2, 14, 20, 23, 24)]
```

What can we conclude substantively from our analysis? First, we can see that some of these topics are easier to interpret than others. Topic 2, for example, clearly centers on school outcomes (school, education, involve, parent). Other topics are a bit harder to parse, and may be an indication that we need to take another look at our model (or text cleaning decisions). For example, Topic 24 is defined by pretty broad, general terms (program, use, data, effect) and is not easy to label. In such cases, it may be useful to examine the abstracts directly to understand why they had been put together. Second, we can see that many of the topics do seem to center on recognizable research areas in the discipline. Topic 23, for example, is about women, gender and sexuality, while Topic 20 focuses on the family. Other topics cover more narrowly defined areas, like Topic 14 that is centered on Japan and investment firms. As a researcher, we would want to walk through the latent topics, interpreting each one and drawing out the larger implications for the substantive case in question. Given these kinds of results, a researcher could also try to predict which researchers fall into which latent topic; for example, based on type of university they attended, gender, etc. Similarly, we could ask how being in a given topic is associated with later career success (i.e., finding a job after graduation).

## A Network Representation
A researcher may also be interested in producing a network representation of the textual data. This makes it possible to apply network measures, models and visualizations to the corpus. So far we have cleaned the data, modeled the data using LDA but we have not explicitly constructed a network object. Here we go ahead and do that, creating a two-mode network of abstracts by words. Let’s load the **igraph** package. 

```{r message=F, warning=F}
library(igraph)
```

We will make use of the abstracts by words matrix extracted earlier. Let’s take a look at the first five rows and columns. The values in the matrix show the number of times that abstract used that word. 

```{r}

mat_abstract_words[1:5, 1:5] 
```

In this case, let’s focus on just a subset of the full matrix. We will look at the network of abstracts and words associated with topic 20 ("family") and topic 23 ("gender").  Thus, we are using the LDA results above (specifically the latent topics) to inform our analysis here. Let’s grab the vector showing which topic each abstract is most likely to be in. 

```{r}
ldaOut_topics2 <- topics(ldaOut2)
```

Now we subset the full abstract-word matrix to just include those abstracts that are most likely to be in topic in 20 or 23. 
```{r}
in_20_23 <- ldaOut_topics2 %in% c(20, 23)
mat_abstract_words_subset <- mat_abstract_words[in_20_23, ]
```

Let’s also reduce the words a bit as well. We will only consider words that are used frequently in topics 20 and 23. This will simplify the picture, eliminating words that are not so relevant for these two topics. Here we calculate the number of times each word was used.

```{r}
worduse <- colSums(mat_abstract_words_subset)
```

Let’s only keep those words that were used more than 5 times.

```{r}
mat_abstract_words_subset <- mat_abstract_words_subset [, worduse > 5]
```

```{r}
dim(mat_abstract_words_subset)
```

We have now reduced our matrix to 24 abstracts and 159 words. We are now in a position to construct the abstract-word network. We will construct a two-mode network, where there are two types of nodes (abstracts and words) and abstracts are connected to words (and vice versa) but there are no direct ties between nodes of the same type. An edge exists if abstract i used word j. We will use the `graph_from_incidence()` function, setting mode to all (creating mutual connections between abstract and words) and keeping the weights (based on the number of times that abstract i used word j). 

```{r}
abstract_word_net <- graph_from_incidence_matrix(mat_abstract_words_subset,
                                                 mode = "all", weighted = T)

```

We will now grab the vertex attribute `type` from the two-mode graph object:

```{r}
type <- vertex_attr(abstract_word_net, "type") 
```

```{r}
table(type)  
```

We see that there are 24 abstracts (FALSE) and 159 words (TRUE). Now, let’s go ahead and plot the network, to see what we can learn from turning the textual data into a network. We first set some plotting parameters. Let’s first set the words green: 

```{r}
V(abstract_word_net)$color[type == TRUE] <- rgb(red = 0, green = 1, 
                                                blue = 0, alpha = .2) 
```

Note that we just change the color for those nodes where `type` is equal to TRUE (the words). Now let’s set the color for the abstracts. Here we want to differentiate the two topics. Let’s color topic 20 (family) blue and topic 23 (gender) red. In order to do this we need to identify which of the abstracts are in topic 20 and which are in topic 23. We must also remember that many of the nodes are words (`type` equal to TRUE). First, let’s get the names of the abstracts in each topic:

```{r}
in20 <- names(which(ldaOut_topics2 == 20))
in23 <- names(which(ldaOut_topics2 == 23))
```

Now we set all those in topic 20 to blue:

```{r}
which_topic20 <- V(abstract_word_net)$name %in% in20
V(abstract_word_net)$color[which_topic20] <- rgb(red = 0, green = 0, 
                                                 blue = 1, alpha = .2)
```

Note that we grabbed the vertex names from the network and then asked which matched those in topic 20. And here we do the same thing with topic 23, setting it to red. 

```{r}
which_topic23 <- V(abstract_word_net)$name %in% in23
V(abstract_word_net)$color[which_topic23] <- rgb(red = 1, green = 0, 
                                                blue = 0, alpha = .2)
```

 Now we'll set some other plotting arguments.
 
```{r}
V(abstract_word_net)$label <- V(abstract_word_net)$name
V(abstract_word_net)$label.color <- rgb(0, 0, .2, .85)
V(abstract_word_net)$label.cex <- .75
V(abstract_word_net)$size <- 3
V(abstract_word_net)$frame.color <- V(abstract_word_net)$color
```

Here we set the color of the edges.

```{r}
E(abstract_word_net)$color <- rgb(.5, .5, .5, .04)
```

And now we plot the network. 

```{r fig.height=9.5, fig.width=9.5}
set.seed(106)
plot(abstract_word_net, layout = layout_with_fr)
```

What can we learn from the graph? First, we can see that the blue nodes (topic 20) tend to cluster together and the red nodes (topic 23) tend to cluster together. This suggests that the topics found in the LDA do appear to capture meaningful distinctions amongst the abstracts. 

Second, we can see that there is a set of core words associated with each topic. We see words like family, children, relationship, parent, and mother cluster together close to the blue nodes, those in topic 20.  We see words like women, gender, and sexuality cluster together close to the red nodes, those in topic 23. This is in line with our previous analysis but here we get the results arranged in a spatial layout. 

Third, we see that some the abstracts are on the periphery of the network. For example, abstract 109 (in topic 23) does not use the words at the core of the network very heavily. Instead, abstract 109 uses words like legal and speech that are rarely used by any of the abstracts in the gender (or family) topic. 

Fourth, the plot makes clear that there is at least some overlap in usage of words across these topics. Most of the blue nodes (the abstracts in topic 20) and the red nodes (abstracts in topic 23) are reasonably close together in the plot and the plot as a whole is quite dense. This suggests that while the usage of the core words is different across topics, there is much in common as well. A family paper is likely to mention gender (or women) even if its main concern is about family, children and the like. On the other hand, the periphery words (i.e. those words that are rarely used) are unlikely to be shared between the two topics. In this way, a topic (or subfield) carves out a niche by emphasizing certain key words that are of general interest, while also employing topic specific language that is unlikely to be shared more widely. 

Given this kind of initial picture, a researcher could go on to do additional analyses on the network, asking about things like centrality, group structure and the like. For example, we may be interested in examining which words serve to bridge our two topics. 

Overall, this tutorial has offered an initial glimpse of how to analyze textual data in R, while offering an example of how duality could be applied to a very different type of data source. We will draw on many of these ideas in the next tutorial for [Chapter 12](#ch12-Networks-Cultural-Spaces-CA-R), where we cover cultural spaces (using correspondence analysis).

# 12, Part 2. Cultural Spaces {.unnumbered #ch12-Networks-Cultural-Spaces-CA-R}

This is the second tutorial for Chapter 12, covering the application of network ideas to cultural data. In this tutorial we cover spatial approaches to analyzing cultural data, which offer a useful, 'birds-eye' view of the system in question. Methods like multi-dimensional scaling and correspondence analysis produce a map of the data, showing which objects cluster together and which do not. We will mostly cover correspondence analysis, which is a method for placing multiple types of objects into the same latent space. Correspondence analysis is thus ideally suited for two-mode data and for capturing ideas of duality. This tutorial thus builds directly on the material covered in [Chapter 11](#ch11-Two-mode-Networks).

Our substantive case is based on the dissertation topics of PhD students in sociology and related disciplines. See [Chapter 12, Part 1](#ch12-Networks-Structure-Culture-text-R) for an analysis of the abstracts themselves. Here, we have data on the topic and department of students who submitted a dissertation between 1990 and 2013 (limited to topics typically studied by sociologists). Our main question is how students in different departments fall into different topics and what structures this relationship. We also want to know which topics are 'close' to which other topics (where topics are close if they are covered by the same kinds of departments) and which departments are close to which other departments (where departments are close if they cover the same kinds of topics). We want to understand the department/topic structure in a holistic manner, capturing the department-department, topic-topic and department-topic relationships simultaneously. It is important to recognize that the data are not network data per se (just counts of people in departments and topics) but we will analyze such data from a relational, network perspective, applying ideas of duality, position, distance and the like.

## Correspondence Analysis
Let's start by loading some useful packages. 

```{r message=F, warning=F}
#install.packages("CAinterprTools")
library(CAinterprTools)
```

```{r message=F, warning=F}
#install.packages("plot3D")
#library(plot3D)
```


```{r message=F, warning=F}
library(FactoMineR)
library(factoextra)
library(gplots)
#library(plot3D)
library(NbClust)
```

Now we will read in the data. 

```{r}
url1 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/department_topic_data.txt"

department_topic <- read.table(file = url1, header = T)
```

Let's take a look at the data (here just for the first 4 rows and columns).

```{r}
department_topic[1:4, 1:4]
```

The rows correspond to the department of the student (with the prefix corresponding to department type: SBS is social behavioral sciences; H is humanities; Ed is the education school; B is business; HS is health sciences). The columns correspond to the main topic of the dissertation. The values are the number of people in that department and topic. So 742 students submitted a dissertation on the life course and were in a sociology department. 

```{r}
dim(department_topic)
```

We have 25 departments and 33 topics. Let’s grab the department and topic names, as this will be useful later on. 

```{r}
department_names <- rownames(department_topic)
topic_names <- colnames(department_topic)
```

We will start with a basic correspondence analysis (CA) to get a sense of what these analyses yield. For a quick primer on correspondence analysis, see [@Sourial2009].

Correspondence analysis will place both the rows (departments) and the columns (topics) into the same space. Departments, for example, will be close together if students from those departments cover the same kinds of topics; i.e., they are structurally equivalent (see [Chapter 10](#ch10-Positions-Roles-R)). [More formally, correspondence analysis works by taking chi-square distances between rows/columns (i.e., a weighted distance between rows based on the probabilities of falling into each column) and decomposing them into a smaller number of dimensions to create a plot where the distances between points approximates those raw chi-square distances.] The main function is `CA()`. The main argument is X, which sets the data frame of interest (here `department_topic`). We also set graph to FALSE as we do not want to plot anything here. 

```{r}
ca_mod1 <- CA(X = department_topic, graph = FALSE)
```

```{r}
ca_mod1
```

The output is a CA object containing the overall fit of the model (`eig`), the results for the columns (`col`) and the results for the rows (`row`). Note that by default up to 5 dimensions will be kept in the results. This can be changed using the ncp argument. Let's use a `summary()` function to take a look at the results. Here we will tell R to print 2 decimal places (using nb.dec), print the first five rows/columns (setting nbelements to 5) and print 3 dimensions (setting ncp to 3). 

```{r}
summary(ca_mod1, nb.dec = 2, nbelements = 5, ncp = 3)
```

These results are a bit hard to interpret but there is lots of useful information here. For example, the eigenvalue output tells us how much of the variance in the data is explained by 1, 2, 3, etc. dimensions. If we use two dimensions (for example) we could explain about 50% of the total variance. We will return to this later on. The row and column output includes (amongst other things) the locations of each department and topic, showing which departments/topics are close together. This is easier to see in a plot. We can plot the results using `fviz_ca_biplot()`.

```{r fig.height=7.5, fig.width=11.25}
fviz_ca_biplot(ca_mod1)
```

We can change some of the inputs to make it easier to interpret, here changing the size and color of the labels. We make departments orange and topics blue.

```{r fig.height=7.5, fig.width=11.25}
fviz_ca_biplot(ca_mod1, col.row = "orange", col.col = "steelblue", 
               labelsize = 3, repel = T) + 
  theme_minimal()
```

Note that the plot only includes the first two dimensions, and that both topics and departments are included, with different colors as a means of differentiating the type of object. Departments are close to topics when those departments have a high proportion of their students doing those topics. For example, students in public health, nursing and medicine tend to take on topics related to medical sociology. 

Substantively, what have we learned from the plot? First, we may be interested in the overall structure of the space, or the latent dimensions in our two dimensional plot. Looking at the x-axis, we see the far left is constituted by topics like child development and deviance and departments like clinical psychology. On the far right, we see topics like social movements and historical sociology and departments like political science and history. The left-right continuum thus runs from micro-level studies of adolescence to macro-level studies of politics and history. Looking at the y-axis, the bottom includes topics like culture and education and includes education and humanities departments. The top is constituted by topics like medical sociology and stratification and departments like economics and public policy. The y-axis thus runs (more or less) from culture to stratification/health. We thus have a two-dimensional space of micro to macro and culture to stratification. 

Second, we may be interested in which topics/departments tend to cluster together. For example, the top left quadrant includes medical sociology topics and public health departments. We see sociology of aging close by, as well as topics related to stratification. We also see that some departments are closer together than others. Public health is closer to nursing and public policy than it is to English or history. This means that the topics that public health students tend to take on are very different than those in the humanities. It is worth noting that the college that these departments are affiliated with only sometimes predicts which department are close together. For example, the departments in the social behavioral sciences (SBS) are pretty spread out, all the way from the far left (social and developmental psychology) to the far right (political science). Thus, political science has more in common, topic wise, with history (in the humanities) than fellow social sciences like clinical psychology.   

We can also clean the plot up a bit to focus on just the rows or just the columns. If we wanted to just have the rows on the plot we would use `fviz_ca_row()` (or `fviz_ca_col()` for just the columns): 

```{r fig.height=7.5, fig.width=11.25}
fviz_ca_row(ca_mod1, col.row = "orange", labelsize = 3, repel = T) +
  theme_minimal()
```

## Number of Dimensions

We have so far run an initial correspondence analysis and interpreted the results. Here we want to take a closer look at the analysis, to make sure we have the proper set of results. There are a number of issues we need to consider. First, we need to decide how many dimensions are sufficient to represent the space. This means we need to decide on the appropriate number of dimensions to use when interpreting the results. We need to balance fit (more dimensions) with ease of interpretation and presentation (fewer dimensions). Remember that the eigenvalues tell us how much each dimension contributes to explaining the total variance in the data. So let's start by looking at the eigenvalues for each dimension more closely.

```{r}
eig <- get_eigenvalue(ca_mod1)
```

```{r}
eig
```

By first inspection, the model would appear to perform okay with a small number of dimensions, although we may be concerned that the first 2 dimensions only explains about 50% of the variance (looking at column 3, `cumulative.variance.percent`). Let's produce a scree plot to make the comparisons clearer.

```{r}
fviz_screeplot(ca_mod1)
```

The y-axis shows the percent of total variance explained as we add a dimension and the x-axis shows the number of dimensions. We can see that additional variance explained seems to drop off after the third dimension, so there would appear to be diminishing returns to adding a fourth, fifth, etc. dimension. We may opt for a 2 or 3 dimensional solution based on the plot. 

Another potentially useful approach is to apply the average rule heuristic, which chooses the number of dimensions by selecting all dimensions that explain more than the average dimension. The function is `aver.rule()` and the input is the raw data (not the CA object). 

```{r}
aver.rule(department_topic)
```

The dashed line indicates the average variance explained and the bars show the percent variance explained for each dimension. The average rule would suggest a solution of 6 dimensions. But if we take a closer look we can see that dimensions 5 and 6 only add around 5% to the explained variance, a fairly low return on adding an entire dimension to the plot. So, we may think that the average rule tends to yield solutions with too many dimensions, at least in this case.  

How about Malinvaud's test? This sequential test checks the significance of the remaining dimensions once the first k ones have been selected. The function is `malinvaud()` and the input is the raw data. 

```{r}
malinvaud(department_topic)
```

The last column of the table prints the significance level associated with that dimension. Lower p-values suggest the (scaled) eigenvalues are significantly different from 0, an indication that the dimension is worth including. The results don't appear to be that informative in this case, however, as all dimensions up to (but not including) 19 are significant (assuming a traditional .05 threshold). 

Given our results, how many dimensions should we use? In this case the scree plot probably offers the most useful information, where a 2 or 3 dimensional solution is the most appropriate. Let’s go ahead and take a look at that third dimension, to see what it adds to the story. Let's create a 3-d plot, focusing just on the departments. Let's first get the locations of our departments. We can find the locations under row and then coord of our CA object: 

```{r}
locs_department <- ca_mod1$row$coord
```

Now, we will use a `text3D()` function to plot the departments in three dimensions. Note that there are a number of packages that will allow for 3d plotting, but here we utilize the functions in the plot3D package. The `text3D()` function has a number of possible arguments. The main arguments are x, y z, showing the locations of each label to be plotted. The labels argument shows what should be printed in each location. Here we set the locations based on the first three dimensions in our CA object and set the labels to the names of the departments, defined above. The rest of the arguments control the look and rotation of the plot. 

## Importance of Rows/Columns
So far we have been examining the quality of the whole model. It will also be useful to assess the specific objects (departments and topics) in terms of their contribution to the fit for each dimension. This can help us in interpreting the dimensions of the space, by showing which rows/columns are most important in fitting a given dimension. Higher values mean that the row is more important in defining that dimension (i.e. the locations of other departments are based on the extreme location of the given department). We can get a handy plot that will sort from high to low on each dimension. The function is `fviv_contrib()`. The arguments are X (the CA object), choice (either "row" or "column") and axes (dimension of interest). Here we plot the contribution of each row for the first dimension. We also change the size and angle of the axis labels to make them easier to read (using ggplot functions).

```{r}
fviz_contrib(ca_mod1, choice = "row", axes = 1) +
  theme(axis.text.x = element_text(size = 8.0, angle = 75))
```

The red line represents the expected value if all rows contributed equally to the dimension. We can see that the first dimension is defined primarily by history, political science and the psychology departments. Rows that contribute a lot to the fit tend to be far from the center on the dimension of interest; in this case history and political science on the far right and psychology on the far left. This is consistent with our interpretation from above. Let's look at dimension 2: 

```{r}
fviz_contrib(ca_mod1, choice = "row", axes = 2) +
    theme(axis.text.x = element_text(size = 8.0, angle = 75)) 
```

And the same thing for the columns:

```{r}
fviz_contrib(ca_mod1, choice = "col", axes = 1) +
  theme(axis.text.x = element_text(size = 8.0, angle = 75)) 

fviz_contrib(ca_mod1, choice = "col", axes = 2) +
  theme(axis.text.x = element_text(size = 8.0, angle = 75)) 
```

Let’s plot the 2-dimensional solution again, but shade the labels by the contribution to the overall fit (summed over the first two dimensions). This is accomplished by setting col.row and col.col to “contrib”. We also include a `scale_color_gradient2()` function to set how the color scale should look.

```{r fig.height=7.5, fig.width=11.25}
fviz_ca_biplot(ca_mod1, col.row = "contrib", col.col ="contrib", 
               labelsize = 3, repel = T) + 
  scale_color_gradient2(low = "white", mid = "steelblue", 
                        high = "darkblue", midpoint = 8) + 
  theme_minimal()
```

Looking at our figure, it is worth thinking about what it means to be in the center of the plot compared to the extremes on one of the dimensions. For example, sociology (department) and gender_sexuality (topic) are in the center, while political science and social movements are not. This would suggest that sociology PhDs tend to take on dissertations that span many of the topics in the field. Similarly, this means that individuals from a wide variety of disciplines study gender. On the other hand, social movements is not studied by a wide variety of departments while those in political science tend not to study a large number of (sociological) topics, focused mainly on social movements. We thus have distinctions between central, generalist departments/topics and those that occupy more peripheral, niche locations. 

## Hierarchical Clustering 
To aid in interpretation, it may be useful to cluster the departments and topics into distinct, structurally equivalent sets, where departments (or topics) that are close together in the space are placed in the same cluster (they are structurally equivalent in the sense of having the same pattern of topics). This can make it easier to talk through the results, as we can start to identify particular regions of the space, and not just the dimensions making up the space. Here we will make use of hierarchical clustering (see also [Chapter 10](#ch10-Positions-Roles-R)). We begin by calculating the distance matrix between objects, both topics and departments, based on the locations from the two dimensional solution. Let's first put locations for the departments and topics together into one data frame. We already have the locations for the departments, so let’s just grab the locations for the topics: 

```{r}
locs_topic <- ca_mod1$col$coord
```

And now we stack them with the locations of the departments using a `rbind()` function: 

```{r}
locs <- rbind(locs_department, locs_topic)
```

We just want the locations for the first two dimensions, so let’s subset the data. 

```{r}
locs <- locs[, 1:2]
```

And now we create the distance matrix using the `dist()` function, setting the method to Euclidean. We will then fit a hierarchical clustering model to the distance matrix using `hclust()`:

```{r}
d <- dist(locs, method = "euclidean") 
fit <- hclust(d, method = "ward.D2") 
```

We need to pick an optimal number of clusters before we can interpret the results. As in [Chapter 10](#ch10-Positions-Roles-R), we can make use of the functions in the **NbClust** package to help us make a choice. The function is `NbClust()`. In this case we just include the distance matrix as the main inputs (set using diss). We set distance to NULL and tell R to use the silhouette index as the criterion (there are a number of other choices we could have made). 

```{r message=F, warning=F, results='hide'}
clusters <- NbClust(data = NULL, diss = d, distance = NULL,
                    method = "ward.D2", index = c("silhouette"))  
```

```{r}
clusters$Best.nc
```

Under this criterion the optimal solution has 8 clusters. The specific clusters can be found under: `clusters$Best.partition`. 

Note that both the topics and the departments are included in the same cluster, so it is a simultaneous clustering of rows/columns. Let's name the clusters in a useful way for the plot. Here we add the word cluster before the cluster number using a `paste()` function.  

```{r}
cluster_names <- paste("cluster", clusters$Best.partition, sep = "") 
```

And let's also put the department/topic names on the vector of clusters so we know what each value corresponds to.

```{r}
names(cluster_names) <- names(clusters$Best.partition)
```

And finally, let's split those out by rows and columns. 

```{r}
row_clusters <- cluster_names[department_names]
col_clusters <- cluster_names[topic_names]
```

Now, let's go back and redo our 2-d plot but color the departments and topics by cluster.

```{r fig.height=7.5, fig.width=11.25}
fviz_ca_biplot(ca_mod1, col.row = row_clusters, col.col = col_clusters, 
               labelsize = 3, repel = T) +
  theme_minimal()
```

Our first impression is that the clusters align pretty well with the spatial layout. There are a few exceptions, where the department or topic is hard to fit (e.g., social theory or special education) but, on the whole, the clusters capture distinct regions of the larger field. We see an education cluster, a health cluster, a culture cluster, a psychology cluster and so on. In this way, we have two main dimensions (micro to macro; culture to stratification) but distinct regions (or niches) carved out within the larger space. These niches are constituted by both topic and department, so that a health cluster (cluster 4) is not merely the topic studied, but institutionally, the departments that tend to study it.  Most illuminating from the cluster analysis, perhaps, is that the center of the figure splits into 2 clusters, divided between sets of 'core' sociology topics: first, we have a cluster including topics like gender and race/ethnicity (cluster 1); and second, we have a cluster containing topics like stratification and organizations (cluster 5).

Note that the clustering solution above is based on clustering objects in a joint space, both row and column objects. It might, however, be preferred to cluster the rows and columns separately (e.g., because we do not want to try and interpret the distance between row and column objects). Here we can rely on the functions from the **FactoMineR** package. We will start by rerunning the correspondence analysis but set ncp to 2 (indicating that only the first 2 dimensions will be retained in the output). We do this to be consistent with the hierarchical clustering analysis done above. 

```{r}
ca_mod2 <- CA(department_topic, ncp = 2, graph = FALSE)
```

We will now use a `HCPC()` function to cluster the departments and topics. In this analysis we will do this separately. We include the CA object as the main input (set using res). The nb.clust argument sets the number of clusters. Here we will set it at 8 to match the analysis above. We could also allow R to find the optimal number of clusters (by setting nb.clust = -1). We also tell R not to plot the results (the default). We set cluster.CA to "rows" to do the rows only.

```{r}
clust_rows <- HCPC(res = ca_mod2, nb.clust = 8,  graph = FALSE, 
                   cluster.CA = "rows")
```

And now for the columns: 

```{r}
clust_cols <- HCPC(res = ca_mod2, nb.clust = 8,  graph = FALSE, 
                   cluster.CA = "columns")
```

Now we plot the correspondence analysis and color the departments by the found cluster. We set choice to "map" to get the factor map (based on the correspondence analysis). We set draw.tree to FALSE as we do not want to plot the hierarchical tree structure. 

```{r fig.height=7.5, fig.width=11.25}
plot(clust_rows, choice = "map", draw.tree = FALSE)
```

It looks pretty similar to what we had before. We can check this by doing a table on the clusters found above (`row_clusters`) and the clusters found here. We need to make sure that the departments are in the same order as above, so the table is meaningful. Here we grab the `clust` variable and sort by `department_names`, before calculating our table.

```{r}
row_clusters2 <- clust_rows$data[department_names, "clust"] 
```

```{r}
table(row_clusters, row_clusters2)
```

We can see that while the labels are different, the departments are placed together in the same way (e.g., cluster 1 is now cluster 4 but the departments are the same).  Overall, the results are identical between the two clustering solutions, so we can be confident in our results. We can also plot the column results.

```{r fig.height=7.5, fig.width=11.25}
plot(clust_cols, choice = "map", draw.tree = FALSE)
```

In the end, we have a holistic representation of an academic field, here based on topics and departments, that can serve as a useful way of analyzing non-network data in a way that is consistent with network ideas. These kinds of analyses push us beyond what we may traditionally think of as the purview of network analysis (actors and the relationships between them) but the ideas of duality, niches, and position are widely applicable, offering important alternatives to traditional regression frameworks. 

